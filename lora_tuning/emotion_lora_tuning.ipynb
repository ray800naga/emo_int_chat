{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM, AutoModelForSeq2SeqLM\n",
    "from datasets import load_dataset\n",
    "\n",
    "from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead\n",
    "from trl.core import LengthSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = PPOConfig(\n",
    "\tmodel_name=\"/workspace/Emotion_Intent_Chat/calm2-7b\",\n",
    "\tlearning_rate=1.41e-5,\n",
    "\tlog_with=\"wandb\"\n",
    ")\n",
    "\n",
    "sent_kwargs = {\"top_k\": None, \"function_to_apply\": \"none\", \"batch_size\": 16}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnaga-lcw-ld-0203\u001b[0m (\u001b[33mnaga-lcw-ld-0203-keio.jp\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/Emotion_Intent_Chat/emo_int_chat/lora_tuning/wandb/run-20240812_003325-j66u7x2f</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/naga-lcw-ld-0203-keio.jp/test_emotion_lora_tuning/runs/j66u7x2f' target=\"_blank\">20240812_003323</a></strong> to <a href='https://wandb.ai/naga-lcw-ld-0203-keio.jp/test_emotion_lora_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/naga-lcw-ld-0203-keio.jp/test_emotion_lora_tuning' target=\"_blank\">https://wandb.ai/naga-lcw-ld-0203-keio.jp/test_emotion_lora_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/naga-lcw-ld-0203-keio.jp/test_emotion_lora_tuning/runs/j66u7x2f' target=\"_blank\">https://wandb.ai/naga-lcw-ld-0203-keio.jp/test_emotion_lora_tuning/runs/j66u7x2f</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/naga-lcw-ld-0203-keio.jp/test_emotion_lora_tuning/runs/j66u7x2f?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f2adca86e60>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "from datetime import datetime\n",
    "\n",
    "current_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "wandb.init(project=f\"test_emotion_lora_tuning\", name=f\"{current_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(config, dataset_name=\"shunk031/wrime\", ver=\"ver1\", input_min_text_length=0, input_max_text_length=8):\n",
    "\ttokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "\t# tokenizer.pad_token = tokenizer.pad_token\n",
    "\ttokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\tds = load_dataset(dataset_name, ver, split=\"train\")\n",
    "\tds = ds.remove_columns([\"user_id\", \"datetime\", \"writer\", \"reader1\", \"reader2\", \"reader3\", \"avg_readers\"])\n",
    "\n",
    "\tdef tokenize(sample):\n",
    "\t\tstc_length = len(tokenizer.encode(sample[\"sentence\"]))\n",
    "\t\tif stc_length < input_max_text_length:\n",
    "\t\t\tinput_size = stc_length\n",
    "\t\telse :\n",
    "\t\t\tinput_size = input_max_text_length\n",
    "\t\tsample[\"input_ids\"] = tokenizer.encode(sample[\"sentence\"])[: input_size]\n",
    "\t\tsample[\"query\"] = tokenizer.decode(sample[\"input_ids\"])\n",
    "\t\treturn sample\n",
    "\n",
    "\tds = ds.map(tokenize, batched=False)\n",
    "\tds.set_format(type=\"torch\")\n",
    "\treturn ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = build_dataset(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collator(data):\n",
    "\treturn dict((key, [d[key] for d in data]) for key in data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import get_peft_config, get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "\ttask_type=TaskType.CAUSAL_LM, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc92006db3204094acddff76d3a9e1b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d46b674b207455b8855b53c8d20350b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load LLM models\n",
    "lora_model = AutoModelForCausalLMWithValueHead.from_pretrained(config.model_name, low_cpu_mem_usage=True, device_map=\"auto\", peft_config=peft_config)\n",
    "# base_model = AutoModelForSeq2SeqLM.from_pretrained(config.model_name, low_cpu_mem_usage=True, device_map=\"auto\")\n",
    "ref_model = AutoModelForCausalLMWithValueHead.from_pretrained(config.model_name, low_cpu_mem_usage=True, device_map=\"auto\")\n",
    "# lora_model = get_peft_model(base_model, peft_config)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.pad_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lora_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|padding|>\n",
      "1\n",
      "<|endoftext|>\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.pad_token)\n",
    "print(tokenizer.pad_token_id)\n",
    "print(tokenizer.eos_token)\n",
    "print(tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:j66u7x2f) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b073bbb2bcc499e8bd756e0470bd3c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">20240812_003323</strong> at: <a href='https://wandb.ai/naga-lcw-ld-0203-keio.jp/test_emotion_lora_tuning/runs/j66u7x2f' target=\"_blank\">https://wandb.ai/naga-lcw-ld-0203-keio.jp/test_emotion_lora_tuning/runs/j66u7x2f</a><br/> View project at: <a href='https://wandb.ai/naga-lcw-ld-0203-keio.jp/test_emotion_lora_tuning' target=\"_blank\">https://wandb.ai/naga-lcw-ld-0203-keio.jp/test_emotion_lora_tuning</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240812_003325-j66u7x2f/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:j66u7x2f). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "468797a7f22443bb80d520cdf981941a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112665762710902, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/Emotion_Intent_Chat/emo_int_chat/lora_tuning/wandb/run-20240812_003732-8iaj68ho</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/naga-lcw-ld-0203-keio.jp/trl/runs/8iaj68ho' target=\"_blank\">bright-dragon-4</a></strong> to <a href='https://wandb.ai/naga-lcw-ld-0203-keio.jp/trl' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/naga-lcw-ld-0203-keio.jp/trl' target=\"_blank\">https://wandb.ai/naga-lcw-ld-0203-keio.jp/trl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/naga-lcw-ld-0203-keio.jp/trl/runs/8iaj68ho' target=\"_blank\">https://wandb.ai/naga-lcw-ld-0203-keio.jp/trl/runs/8iaj68ho</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# initialize PPOTrainer\n",
    "ppo_trainer = PPOTrainer(config, lora_model, ref_model, tokenizer, dataset=dataset, data_collator=collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = ppo_trainer.accelerator.device\n",
    "if ppo_trainer.accelerator.num_processes == 1:\n",
    "\tdevice = 0 if torch.cuda.is_available() else \"cpu\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_pipe = pipeline(\"text-classification\", model=\"/workspace/Emotion_Intent_Chat/emo_int_chat/emotion_reward_model/tuned_model/20240801_044013_bert-base-japanese-v3_cosine_with_restarts/checkpoint-9621\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'Anticipation', 'score': 1.488558292388916},\n",
       " {'label': 'Joy', 'score': -0.3886352479457855},\n",
       " {'label': 'Trust', 'score': -3.6500744819641113},\n",
       " {'label': 'Sadness', 'score': -4.7005510330200195},\n",
       " {'label': 'Disgust', 'score': -4.830318450927734},\n",
       " {'label': 'Fear', 'score': -5.197143077850342},\n",
       " {'label': 'Surprise', 'score': -5.298903465270996},\n",
       " {'label': 'Anger', 'score': -5.537960529327393}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"彼はとてもたよりになる\"\n",
    "emotion_pipe(text, **sent_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_kwargs = {\"min_length\": 0, \"top_k\": 500, \"top_p\": 0.95, \"do_sample\": True, \"pad_token_id\": tokenizer.eos_token_id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select emotion\n",
    "emotion = \"Trust\"\n",
    "\n",
    "emotion_dict = {\n",
    "    \"Joy\": 0,\n",
    "    \"Sadness\": 1,\n",
    "    \"Anticipation\": 2,\n",
    "    \"Surprise\": 3,\n",
    "    \"Anger\": 4,\n",
    "    \"Fear\": 5,\n",
    "    \"Disgust\": 6,\n",
    "    \"Trust\": 7\n",
    "}\n",
    "\n",
    "emotion_id = emotion_dict[emotion]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "312it [6:49:28, 78.74s/it]\n"
     ]
    }
   ],
   "source": [
    "generation_kwargs = {\n",
    "    \"min_length\": -1,\n",
    "    # \"min_length\": 0,\n",
    "    \"top_k\": 500,\n",
    "    \"top_p\": 0.95,\n",
    "    \"do_sample\": True,\n",
    "    # \"pad_token_id\": tokenizer.pad_token_id,\n",
    "    \"pad_token_id\": tokenizer.eos_token_id,\n",
    "    \"temperature\": 1\n",
    "}\n",
    "\n",
    "\n",
    "for epoch, batch in tqdm(enumerate(ppo_trainer.dataloader)):\n",
    "    query_tensors = batch[\"input_ids\"]\n",
    "\n",
    "    #### Get response from calm\n",
    "    response_tensors = []\n",
    "    for query in query_tensors:\n",
    "        # gen_len = output_length_sampler()\n",
    "        gen_len = 10\n",
    "        generation_kwargs[\"max_new_tokens\"] = gen_len\n",
    "        response = ppo_trainer.generate(query, **generation_kwargs)\n",
    "        if len(response.squeeze()) < gen_len:\n",
    "            gen_len = len(response.squeeze())\n",
    "        response_tensors.append(response.squeeze()[-gen_len:])\n",
    "    batch[\"response\"] = [tokenizer.decode(r.squeeze()) for r in response_tensors]\n",
    "\n",
    "    #### Compute sentiment score\n",
    "    texts = [q + r for q, r in zip(batch[\"query\"], batch[\"response\"])]\n",
    "    pipe_outputs = emotion_pipe(texts, **sent_kwargs)\n",
    "    rewards = [torch.tensor(output[emotion_id][\"score\"]) for output in pipe_outputs]\n",
    "\n",
    "    #### Run PPO step\n",
    "    stats = ppo_trainer.step(query_tensors, response_tensors, rewards)\n",
    "    ppo_trainer.log_stats(stats, batch, rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>response (before)</th>\n",
       "      <th>response (after)</th>\n",
       "      <th>rewards (before)</th>\n",
       "      <th>rewards (after)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(´-`).｡oO(</td>\n",
       "      <td>あちち。。)\\nん？\\nなぜか</td>\n",
       "      <td>そんなことしないですっ・・・)\\nそんなことはさて</td>\n",
       "      <td>-5.756476</td>\n",
       "      <td>-5.992166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>行ったことのない県あったっけ...？(</td>\n",
       "      <td>混乱)\\n愛知県、岐阜県、三重県、和歌山県</td>\n",
       "      <td>ﾟдﾟ)\\n■　４月</td>\n",
       "      <td>-5.020241</td>\n",
       "      <td>-5.992216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>リリックビデオ作ってみたい。</td>\n",
       "      <td>\\nこのリリックビデオを作った人は、どうして映画</td>\n",
       "      <td>\\nコロナが憎いです。\\n本当は会いたい</td>\n",
       "      <td>-5.372670</td>\n",
       "      <td>-5.012854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ハウルってぐぅイケメンだしイケ</td>\n",
       "      <td>ボ(イケメンボイスね)だし、</td>\n",
       "      <td>メンが魔法使いになった時点ですごい\\nん</td>\n",
       "      <td>-5.728635</td>\n",
       "      <td>-4.454085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>野田に住んでるといきなり霊波</td>\n",
       "      <td>之光の黒い御揃いのスーツの男性たちに</td>\n",
       "      <td>が出てビックリ。なんてこともあるらしい」とか言われて驚いた</td>\n",
       "      <td>-4.997355</td>\n",
       "      <td>-4.445333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>久しぶりにテニミュの曲聴いてる。\\</td>\n",
       "      <td>n\\n09-10 20</td>\n",
       "      <td>n懐かしさで涙出てきた。pic.twitter.</td>\n",
       "      <td>-6.126385</td>\n",
       "      <td>-5.902471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ご家庭で声かけお願いします。ご家庭</td>\n",
       "      <td>の方の声かけで、時間厳守を心がけて</td>\n",
       "      <td>でも少しでもいいから、子どもに関心を持ってあげてください</td>\n",
       "      <td>-5.069574</td>\n",
       "      <td>-4.885078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7割くらいのスペース使ってないけど</td>\n",
       "      <td>、\\n2016年10月</td>\n",
       "      <td>。」\\nと言っていました。\\n私はそれが凄く不思議</td>\n",
       "      <td>-5.972430</td>\n",
       "      <td>-4.410717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>工場で派遣やった時に出稼ぎの中国人から</td>\n",
       "      <td>半日１週間早出してくれや！と言われた</td>\n",
       "      <td>聞いたわ\\n何ヶ月か働いてきて中国人だと</td>\n",
       "      <td>-5.317713</td>\n",
       "      <td>-4.111549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>一色いろはに完全にやられた</td>\n",
       "      <td>のか、俺は見てないと言い出して...\\nもし</td>\n",
       "      <td>、これ凄いな\\n今さっきびっくりしたことが起きた</td>\n",
       "      <td>-5.065411</td>\n",
       "      <td>-4.241178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>めしガチャに中之条のC</td>\n",
       "      <td>afé onionの新メニュー「夢のそぼ</td>\n",
       "      <td>さんが来てくれたのはびっくりしました。\\nそして、</td>\n",
       "      <td>-6.698995</td>\n",
       "      <td>-4.298154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>(朗報) 娘、寝る</td>\n",
       "      <td>直前にめちゃくちゃ嫌そうな顔して頑張って寝ようとして</td>\n",
       "      <td>前にチョコレート食べて！と泣き叫んで叫</td>\n",
       "      <td>-5.848172</td>\n",
       "      <td>-5.598909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>焼き鮭と政宗くんのリベ</td>\n",
       "      <td>ンジです。\\nこの主人公、政宗君</td>\n",
       "      <td>ンジはいい漫画なのに\\n仙台放送局もスクランブル</td>\n",
       "      <td>-5.509706</td>\n",
       "      <td>-5.280305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>スマホの緊急地震速報初めて受信した</td>\n",
       "      <td>！スマホのデータ通信に異常が出た・・、\\n</td>\n",
       "      <td>けど、驚いてちょっと焦ったわ あの</td>\n",
       "      <td>-4.721638</td>\n",
       "      <td>-4.487796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ママ友に関して、量より質を求める</td>\n",
       "      <td>ようになった。\\nとはいえ、質を求めるのは理想</td>\n",
       "      <td>ママ友は嫌味でうざいと感じ</td>\n",
       "      <td>-6.076225</td>\n",
       "      <td>-4.273739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>今のではいらんかーーー！</td>\n",
       "      <td>と思ってしまいます。\\n今までたくさん、色々と、お世話</td>\n",
       "      <td>まさか他の奴らが同類とはよもや思い</td>\n",
       "      <td>-5.814283</td>\n",
       "      <td>-4.436425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>都立って普通の公立全部ってこと？</td>\n",
       "      <td>ちゃんと確認してくださいね。\\n本当のところ教えて</td>\n",
       "      <td>すごい発想だったんですね！」って驚く人は、うち</td>\n",
       "      <td>-5.438141</td>\n",
       "      <td>-3.971120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>「生きろ、そなたは美しい</td>\n",
       "      <td>」というのは「しっかり生きるんだぞ」とか「</td>\n",
       "      <td>」と熱唱！\\nコンサートは2人の</td>\n",
       "      <td>-4.965086</td>\n",
       "      <td>-5.365684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>チャラい子嫌いではないけどさ</td>\n",
       "      <td>、それでもこのタイミングでそういうこと言う子はちょっと</td>\n",
       "      <td>\\nモテそうな人に、告白されても、うわ！</td>\n",
       "      <td>-4.884524</td>\n",
       "      <td>-5.693717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>やっすいカラコンにしたら、</td>\n",
       "      <td>目が悪くなるよ。\\nおそ松は</td>\n",
       "      <td>怖かった。\\n今考えると不思議だな・・・笑</td>\n",
       "      <td>-5.430975</td>\n",
       "      <td>-4.697009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>弊研究室 緩いわけではなく単純に</td>\n",
       "      <td>キャパ不足という理由で１人か2人に</td>\n",
       "      <td>そこまで真剣に教えていないだけで、君を舐めている</td>\n",
       "      <td>-5.884770</td>\n",
       "      <td>-5.063345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>レッサーパンダちゃんかわいい</td>\n",
       "      <td>ですよね。\\nでも、やっぱりオオコウモリとは</td>\n",
       "      <td>よ！かわいい！\\n【C95】</td>\n",
       "      <td>-6.282770</td>\n",
       "      <td>-5.503038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>鍵山くんよかったあああ</td>\n",
       "      <td>！！！！\\nそして１位取りきりオオカミ</td>\n",
       "      <td>！これは絶対見ないと！すごい期待しています！</td>\n",
       "      <td>-5.415579</td>\n",
       "      <td>-5.528849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>いろんなことで混乱してるうちに絵の仕上げ</td>\n",
       "      <td>ちゃったよｗｗｗ 後は、仕上げた絵</td>\n",
       "      <td>まで時間かかりすぎ。\\nしかし、今日こんな</td>\n",
       "      <td>-5.580188</td>\n",
       "      <td>-5.313046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>久々に出社して上司先輩と話したら</td>\n",
       "      <td>帰ろっかな？って思っちゃう。仕事よりも</td>\n",
       "      <td>、何だか自分がめっちゃ怒られてい、なんか調子</td>\n",
       "      <td>-5.904830</td>\n",
       "      <td>-5.775031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>コンビニで買ったファンデより100</td>\n",
       "      <td>0円以上安く済むのがポイント高い。\\n最近</td>\n",
       "      <td>円安いのに、びっくり！！！なんか塗るたび</td>\n",
       "      <td>-5.478458</td>\n",
       "      <td>-4.163571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>チュッキューさんのプロフに独身</td>\n",
       "      <td>がいないのは、不思議な話ではあります。出会いがない</td>\n",
       "      <td>と書いてあったのはビックリしました！\\nなぜそんな風に</td>\n",
       "      <td>-5.261582</td>\n",
       "      <td>-4.165246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>わあ、クルードラゴンの打ち</td>\n",
       "      <td>上げなんて見たことないなあと思って、YouTubeやリアル</td>\n",
       "      <td>上げすっごい楽しみ。日本もぜひ乗せ</td>\n",
       "      <td>-5.111887</td>\n",
       "      <td>-5.214295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>そろそろ「ネットでしか関西弁使ってなさ</td>\n",
       "      <td>そうな人」が想像以上に多い気がするな\\n</td>\n",
       "      <td>そう」て言われててびっくりしたわ。「俺</td>\n",
       "      <td>-5.186845</td>\n",
       "      <td>-4.316932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>草野球大会したい</td>\n",
       "      <td>な～。」っと盛り上がりを見せたところで終了です。</td>\n",
       "      <td>！\\n日本シリーズやなんJではまた1</td>\n",
       "      <td>-5.551678</td>\n",
       "      <td>-4.979270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>ジムたんよりも燃費がいいことを理由に片</td>\n",
       "      <td>道３時間はバイクを降りたくな（乗り換え</td>\n",
       "      <td>嶋さんに運転してもらうしかないです！！\\n続いて6</td>\n",
       "      <td>-6.101557</td>\n",
       "      <td>-5.363473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>「お母さん、あのね、大石は</td>\n",
       "      <td>ね、結婚の時に、家出しちゃって、</td>\n",
       "      <td>殺してないんだよ！！！」と叫んで怒鳴</td>\n",
       "      <td>-5.232671</td>\n",
       "      <td>-4.369870</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   query              response (before)  \\\n",
       "0             (´-`).｡oO(                あちち。。)\\nん？\\nなぜか   \n",
       "1    行ったことのない県あったっけ...？(          混乱)\\n愛知県、岐阜県、三重県、和歌山県   \n",
       "2         リリックビデオ作ってみたい。       \\nこのリリックビデオを作った人は、どうして映画   \n",
       "3        ハウルってぐぅイケメンだしイケ                 ボ(イケメンボイスね)だし、   \n",
       "4         野田に住んでるといきなり霊波             之光の黒い御揃いのスーツの男性たちに   \n",
       "5      久しぶりにテニミュの曲聴いてる。\\                    n\\n09-10 20   \n",
       "6      ご家庭で声かけお願いします。ご家庭              の方の声かけで、時間厳守を心がけて   \n",
       "7      7割くらいのスペース使ってないけど                    、\\n2016年10月   \n",
       "8    工場で派遣やった時に出稼ぎの中国人から             半日１週間早出してくれや！と言われた   \n",
       "9          一色いろはに完全にやられた         のか、俺は見てないと言い出して...\\nもし   \n",
       "10           めしガチャに中之条のC           afé onionの新メニュー「夢のそぼ   \n",
       "11             (朗報) 娘、寝る     直前にめちゃくちゃ嫌そうな顔して頑張って寝ようとして   \n",
       "12           焼き鮭と政宗くんのリベ               ンジです。\\nこの主人公、政宗君   \n",
       "13     スマホの緊急地震速報初めて受信した          ！スマホのデータ通信に異常が出た・・、\\n   \n",
       "14      ママ友に関して、量より質を求める        ようになった。\\nとはいえ、質を求めるのは理想   \n",
       "15          今のではいらんかーーー！    と思ってしまいます。\\n今までたくさん、色々と、お世話   \n",
       "16      都立って普通の公立全部ってこと？      ちゃんと確認してくださいね。\\n本当のところ教えて   \n",
       "17          「生きろ、そなたは美しい          」というのは「しっかり生きるんだぞ」とか「   \n",
       "18        チャラい子嫌いではないけどさ    、それでもこのタイミングでそういうこと言う子はちょっと   \n",
       "19         やっすいカラコンにしたら、                 目が悪くなるよ。\\nおそ松は   \n",
       "20      弊研究室 緩いわけではなく単純に              キャパ不足という理由で１人か2人に   \n",
       "21        レッサーパンダちゃんかわいい         ですよね。\\nでも、やっぱりオオコウモリとは   \n",
       "22           鍵山くんよかったあああ            ！！！！\\nそして１位取りきりオオカミ   \n",
       "23  いろんなことで混乱してるうちに絵の仕上げ              ちゃったよｗｗｗ 後は、仕上げた絵   \n",
       "24      久々に出社して上司先輩と話したら            帰ろっかな？って思っちゃう。仕事よりも   \n",
       "25     コンビニで買ったファンデより100          0円以上安く済むのがポイント高い。\\n最近   \n",
       "26       チュッキューさんのプロフに独身      がいないのは、不思議な話ではあります。出会いがない   \n",
       "27         わあ、クルードラゴンの打ち  上げなんて見たことないなあと思って、YouTubeやリアル   \n",
       "28   そろそろ「ネットでしか関西弁使ってなさ           そうな人」が想像以上に多い気がするな\\n   \n",
       "29              草野球大会したい       な～。」っと盛り上がりを見せたところで終了です。   \n",
       "30   ジムたんよりも燃費がいいことを理由に片            道３時間はバイクを降りたくな（乗り換え   \n",
       "31         「お母さん、あのね、大石は               ね、結婚の時に、家出しちゃって、   \n",
       "\n",
       "                 response (after)  rewards (before)  rewards (after)  \n",
       "0       そんなことしないですっ・・・)\\nそんなことはさて         -5.756476        -5.992166  \n",
       "1                     　ﾟдﾟ)\\n■　４月         -5.020241        -5.992216  \n",
       "2            \\nコロナが憎いです。\\n本当は会いたい         -5.372670        -5.012854  \n",
       "3            メンが魔法使いになった時点ですごい\\nん         -5.728635        -4.454085  \n",
       "4   が出てビックリ。なんてこともあるらしい」とか言われて驚いた         -4.997355        -4.445333  \n",
       "5        n懐かしさで涙出てきた。pic.twitter.         -6.126385        -5.902471  \n",
       "6    でも少しでもいいから、子どもに関心を持ってあげてください         -5.069574        -4.885078  \n",
       "7       。」\\nと言っていました。\\n私はそれが凄く不思議         -5.972430        -4.410717  \n",
       "8            聞いたわ\\n何ヶ月か働いてきて中国人だと         -5.317713        -4.111549  \n",
       "9        、これ凄いな\\n今さっきびっくりしたことが起きた         -5.065411        -4.241178  \n",
       "10      さんが来てくれたのはびっくりしました。\\nそして、         -6.698995        -4.298154  \n",
       "11            前にチョコレート食べて！と泣き叫んで叫         -5.848172        -5.598909  \n",
       "12       ンジはいい漫画なのに\\n仙台放送局もスクランブル         -5.509706        -5.280305  \n",
       "13              けど、驚いてちょっと焦ったわ あの         -4.721638        -4.487796  \n",
       "14                  ママ友は嫌味でうざいと感じ         -6.076225        -4.273739  \n",
       "15              まさか他の奴らが同類とはよもや思い         -5.814283        -4.436425  \n",
       "16        すごい発想だったんですね！」って驚く人は、うち         -5.438141        -3.971120  \n",
       "17               」と熱唱！\\nコンサートは2人の         -4.965086        -5.365684  \n",
       "18           \\nモテそうな人に、告白されても、うわ！         -4.884524        -5.693717  \n",
       "19          怖かった。\\n今考えると不思議だな・・・笑         -5.430975        -4.697009  \n",
       "20       そこまで真剣に教えていないだけで、君を舐めている         -5.884770        -5.063345  \n",
       "21                 よ！かわいい！\\n【C95】         -6.282770        -5.503038  \n",
       "22         ！これは絶対見ないと！すごい期待しています！         -5.415579        -5.528849  \n",
       "23          まで時間かかりすぎ。\\nしかし、今日こんな         -5.580188        -5.313046  \n",
       "24         、何だか自分がめっちゃ怒られてい、なんか調子         -5.904830        -5.775031  \n",
       "25           円安いのに、びっくり！！！なんか塗るたび         -5.478458        -4.163571  \n",
       "26    と書いてあったのはビックリしました！\\nなぜそんな風に         -5.261582        -4.165246  \n",
       "27              上げすっごい楽しみ。日本もぜひ乗せ         -5.111887        -5.214295  \n",
       "28            そう」て言われててびっくりしたわ。「俺         -5.186845        -4.316932  \n",
       "29             ！\\n日本シリーズやなんJではまた1         -5.551678        -4.979270  \n",
       "30      嶋さんに運転してもらうしかないです！！\\n続いて6         -6.101557        -5.363473  \n",
       "31             殺してないんだよ！！！」と叫んで怒鳴         -5.232671        -4.369870  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### get a batch from the dataset\n",
    "bs = 32\n",
    "game_data = dict()\n",
    "dataset.set_format(\"pandas\")\n",
    "df_batch = dataset[:].sample(bs)\n",
    "game_data[\"query\"] = df_batch[\"query\"].tolist()\n",
    "query_tensors = df_batch[\"input_ids\"].tolist()\n",
    "\n",
    "response_tensors_ref, response_tensors = [], []\n",
    "\n",
    "#### get response from gpt2 and gpt2_ref\n",
    "for i in range(bs):\n",
    "    # gen_len = output_length_sampler()\n",
    "    gen_len = 10\n",
    "    output = ref_model.generate(\n",
    "        torch.tensor(query_tensors[i]).unsqueeze(dim=0).to(device), max_new_tokens=gen_len, **gen_kwargs\n",
    "    ).squeeze()[-gen_len:]\n",
    "    response_tensors_ref.append(output)\n",
    "    output = lora_model.generate(\n",
    "        torch.tensor(query_tensors[i]).unsqueeze(dim=0).to(device), max_new_tokens=gen_len, **gen_kwargs\n",
    "    ).squeeze()[-gen_len:]\n",
    "    response_tensors.append(output)\n",
    "\n",
    "#### decode responses\n",
    "game_data[\"response (before)\"] = [tokenizer.decode(response_tensors_ref[i]) for i in range(bs)]\n",
    "game_data[\"response (after)\"] = [tokenizer.decode(response_tensors[i]) for i in range(bs)]\n",
    "\n",
    "#### sentiment analysis of query/response pairs before/after\n",
    "texts = [q + r for q, r in zip(game_data[\"query\"], game_data[\"response (before)\"])]\n",
    "game_data[\"rewards (before)\"] = [output[emotion_id][\"score\"] for output in emotion_pipe(texts, **sent_kwargs)]\n",
    "\n",
    "texts = [q + r for q, r in zip(game_data[\"query\"], game_data[\"response (after)\"])]\n",
    "game_data[\"rewards (after)\"] = [output[emotion_id][\"score\"] for output in emotion_pipe(texts, **sent_kwargs)]\n",
    "\n",
    "# store results in a dataframe\n",
    "df_results = pd.DataFrame(game_data)\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rewards (before)   -5.525233\n",
       "rewards (after)    -4.915827\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "median:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rewards (before)   -5.458299\n",
       "rewards (after)    -4.932174\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"mean:\")\n",
    "display(df_results[[\"rewards (before)\", \"rewards (after)\"]].mean())\n",
    "print()\n",
    "print(\"median:\")\n",
    "display(df_results[[\"rewards (before)\", \"rewards (after)\"]].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/workspace/Emotion_Intent_Chat/emo_int_chat/lora_tuning/tuned_model/calm2-7b-Trust/tokenizer_config.json',\n",
       " '/workspace/Emotion_Intent_Chat/emo_int_chat/lora_tuning/tuned_model/calm2-7b-Trust/special_tokens_map.json',\n",
       " '/workspace/Emotion_Intent_Chat/emo_int_chat/lora_tuning/tuned_model/calm2-7b-Trust/tokenizer.json')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "save_dir = f\"/workspace/Emotion_Intent_Chat/emo_int_chat/lora_tuning/tuned_model/{config.model_name.split('/')[-1]}-{emotion}\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "lora_model.save_pretrained(save_dir, push_to_hub=False)\n",
    "tokenizer.save_pretrained(save_dir, push_to_hub=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
